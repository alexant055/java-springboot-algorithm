<?xml version="1.0" encoding="UTF-8" standalone="no"?><configuration>
<property><name>job.end.retry.interval</name><value>30000</value><source>programatically</source></property>
<property><name>io.bytes.per.checksum</name><value>512</value><source>programatically</source></property>
<property><name>mapred.job.tracker.retiredjobs.cache.size</name><value>1000</value><source>programatically</source></property>
<property><name>dfs.image.transfer.bandwidthPerSec</name><value>0</value><source>programatically</source></property>
<property><name>mapred.task.profile.reduces</name><value>0-2</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.staging.root.dir</name><value>${hadoop.tmp.dir}/mapred/staging</value><source>programatically</source></property>
<property><name>eclipse.plug-in.jobtracker.port</name><value>54311</value><source>programatically</source></property>
<property><name>mapred.job.reuse.jvm.num.tasks</name><value>1</value><source>programatically</source></property>
<property><name>dfs.block.access.token.lifetime</name><value>600</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.file.impl</name><value>org.apache.hadoop.fs.local.LocalFs</value><source>programatically</source></property>
<property><name>mapred.reduce.tasks.speculative.execution</name><value>true</value><source>programatically</source></property>
<property><name>fs.du.interval</name><value>600000</value><source>programatically</source></property>
<property><name>hadoop.ssl.keystores.factory.class</name><value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.kerberos.keytab</name><value>${user.home}/hadoop.keytab</value><source>programatically</source></property>
<property><name>dfs.permissions.supergroup</name><value>supergroup</value><source>programatically</source></property>
<property><name>io.seqfile.sorter.recordlimit</name><value>1000000</value><source>programatically</source></property>
<property><name>ipc.client.connect.retry.interval</name><value>1000</value><source>programatically</source></property>
<property><name>s3.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>hadoop.relaxed.worker.version.check</name><value>false</value><source>programatically</source></property>
<property><name>mapred.task.tracker.http.address</name><value>0.0.0.0:50060</value><source>programatically</source></property>
<property><name>dfs.namenode.delegation.token.renew-interval</name><value>86400000</value><source>programatically</source></property>
<property><name>io.map.index.interval</name><value>128</value><source>programatically</source></property>
<property><name>nfs3.mountd.port</name><value>4242</value><source>programatically</source></property>
<property><name>s3.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>ha.zookeeper.session-timeout.ms</name><value>5000</value><source>programatically</source></property>
<property><name>fs.ramfs.impl</name><value>org.apache.hadoop.fs.InMemoryFileSystem</value><source>programatically</source></property>
<property><name>mapred.system.dir</name><value>${hadoop.tmp.dir}/mapred/system</value><source>programatically</source></property>
<property><name>mapred.remote.os</name><value>Linux</value><source>programatically</source></property>
<property><name>eclipse.plug-in.masters.colocate</name><value>no</value><source>programatically</source></property>
<property><name>s3.replication</name><value>3</value><source>programatically</source></property>
<property><name>mapred.task.tracker.report.address</name><value>127.0.0.1:0</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.connect.timeout</name><value>180000</value><source>programatically</source></property>
<property><name>hadoop.ssl.enabled</name><value>false</value><source>programatically</source></property>
<property><name>eclipse.plug-in.location.name</name><value>Hadoop AD</value><source>programatically</source></property>
<property><name>mapreduce.job.counters.max</name><value>120</value><source>programatically</source></property>
<property><name>hadoop.security.groups.cache.warn.after.ms</name><value>5000</value><source>programatically</source></property>
<property><name>ipc.client.connect.max.retries.on.timeouts</name><value>45</value><source>programatically</source></property>
<property><name>mapred.healthChecker.interval</name><value>60000</value><source>programatically</source></property>
<property><name>mapreduce.job.complete.cancel.delegation.tokens</name><value>true</value><source>programatically</source></property>
<property><name>dfs.namenode.replication.work.multiplier.per.iteration</name><value>2</value><source>programatically</source></property>
<property><name>fs.trash.interval</name><value>0</value><source>programatically</source></property>
<property><name>ha.health-monitor.check-interval.ms</name><value>1000</value><source>programatically</source></property>
<property><name>hadoop.jetty.logs.serve.aliases</name><value>true</value><source>programatically</source></property>
<property><name>mapred.skip.map.auto.incr.proc.count</name><value>true</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.kerberos.principal</name><value>HTTP/localhost@LOCALHOST</value><source>programatically</source></property>
<property><name>s3native.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>mapred.child.tmp</name><value>./tmp</value><source>programatically</source></property>
<property><name>mapred.tasktracker.taskmemorymanager.monitoring-interval</name><value>5000</value><source>programatically</source></property>
<property><name>ha.health-monitor.sleep-after-disconnect.ms</name><value>1000</value><source>programatically</source></property>
<property><name>dfs.datanode.http.address</name><value>0.0.0.0:50075</value><source>programatically</source></property>
<property><name>io.sort.spill.percent</name><value>0.80</value><source>programatically</source></property>
<property><name>dfs.client.use.datanode.hostname</name><value>false</value><source>programatically</source></property>
<property><name>mapred.job.shuffle.input.buffer.percent</name><value>0.70</value><source>programatically</source></property>
<property><name>dfs.max.objects</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.security.instrumentation.requires.admin</name><value>false</value><source>programatically</source></property>
<property><name>mapred.skip.map.max.skip.records</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.maxfetchfailures</name><value>10</value><source>programatically</source></property>
<property><name>hadoop.security.authorization</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.filter.group</name><value>(objectClass=group)</value><source>programatically</source></property>
<property><name>eclipse.plug-in.socks.proxy.host</name><value>host</value><source>programatically</source></property>
<property><name>mapred.task.profile.maps</name><value>0-2</value><source>programatically</source></property>
<property><name>mapreduce.app-submission.cross-platform</name><value>true</value><source>programatically</source></property>
<property><name>dfs.https.server.keystore.resource</name><value>ssl-server.xml</value><source>programatically</source></property>
<property><name>dfs.replication.interval</name><value>3</value><source>programatically</source></property>
<property><name>mapred.local.dir</name><value>${hadoop.tmp.dir}/mapred/local</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.attr.group.name</name><value>cn</value><source>programatically</source></property>
<property><name>mapred.merge.recordsBeforeProgress</name><value>10000</value><source>programatically</source></property>
<property><name>mapred.job.tracker.http.address</name><value>0.0.0.0:50030</value><source>programatically</source></property>
<property><name>fs.client.resolve.remote.symlinks</name><value>true</value><source>programatically</source></property>
<property><name>mapred.compress.map.output</name><value>false</value><source>programatically</source></property>
<property><name>mapred.userlog.retain.hours</name><value>24</value><source>programatically</source></property>
<property><name>s3native.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>tfile.fs.output.buffer.size</name><value>262144</value><source>programatically</source></property>
<property><name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.hdfs.impl</name><value>org.apache.hadoop.fs.Hdfs</value><source>programatically</source></property>
<property><name>dfs.namenode.safemode.min.datanodes</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.security.uid.cache.secs</name><value>14400</value><source>programatically</source></property>
<property><name>mapred.disk.healthChecker.interval</name><value>60000</value><source>programatically</source></property>
<property><name>fs.har.impl.disable.cache</name><value>true</value><source>programatically</source></property>
<property><name>mapred.cluster.map.memory.mb</name><value>-1</value><source>programatically</source></property>
<property><name>hadoop.ssl.client.conf</name><value>ssl-client.xml</value><source>programatically</source></property>
<property><name>dfs.data.dir</name><value>${hadoop.tmp.dir}/dfs/data</value><source>programatically</source></property>
<property><name>dfs.access.time.precision</name><value>3600000</value><source>programatically</source></property>
<property><name>dfs.replication.min</name><value>1</value><source>programatically</source></property>
<property><name>fs.checkpoint.dir</name><value>${hadoop.tmp.dir}/dfs/namesecondary</value><source>programatically</source></property>
<property><name>fs.s3n.impl</name><value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.outofband.heartbeat</name><value>false</value><source>programatically</source></property>
<property><name>fs.s3n.multipart.uploads.enabled</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.outofband.heartbeat.damper</name><value>1000000</value><source>programatically</source></property>
<property><name>io.native.lib.available</name><value>true</value><source>because hadoop.native.lib is deprecated</source></property>
<property><name>mapred.jobtracker.restart.recover</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.user.group.static.mapping.overrides</name><value>dr.who=;</value><source>programatically</source></property>
<property><name>hadoop.logfile.size</name><value>10000000</value><source>programatically</source></property>
<property><name>hadoop.security.token.service.use_ip</name><value>true</value><source>programatically</source></property>
<property><name>mapred.inmem.merge.threshold</name><value>1000</value><source>programatically</source></property>
<property><name>ipc.client.connection.maxidletime</name><value>10000</value><source>programatically</source></property>
<property><name>fs.checkpoint.size</name><value>67108864</value><source>programatically</source></property>
<property><name>dfs.namenode.invalidate.work.pct.per.iteration</name><value>0.32f</value><source>programatically</source></property>
<property><name>dfs.blockreport.intervalMsec</name><value>3600000</value><source>programatically</source></property>
<property><name>fs.s3.sleepTimeSeconds</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.job.counters.counter.name.max</name><value>64</value><source>programatically</source></property>
<property><name>dfs.client.block.write.retries</name><value>3</value><source>programatically</source></property>
<property><name>hadoop.ssl.server.conf</name><value>ssl-server.xml</value><source>programatically</source></property>
<property><name>fs.s3n.multipart.uploads.block.size</name><value>67108864</value><source>programatically</source></property>
<property><name>mapred.reduce.tasks</name><value>1</value><source>programatically</source></property>
<property><name>ha.zookeeper.parent-znode</name><value>/hadoop-ha</value><source>programatically</source></property>
<property><name>mapred.queue.names</name><value>default</value><source>programatically</source></property>
<property><name>io.seqfile.lazydecompress</name><value>true</value><source>programatically</source></property>
<property><name>dfs.https.enable</name><value>false</value><source>programatically</source></property>
<property><name>dfs.replication</name><value>3</value><source>programatically</source></property>
<property><name>mapred.jobtracker.blacklist.fault-timeout-window</name><value>180</value><source>programatically</source></property>
<property><name>ipc.client.tcpnodelay</name><value>false</value><source>programatically</source></property>
<property><name>eclipse.plug-in.user.name</name><value>axa8047</value><source>programatically</source></property>
<property><name>mapred.acls.enabled</name><value>false</value><source>programatically</source></property>
<property><name>s3.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>mapred.tasktracker.dns.nameserver</name><value>default</value><source>programatically</source></property>
<property><name>mapred.submit.replication</name><value>10</value><source>programatically</source></property>
<property><name>nfs3.server.port</name><value>2049</value><source>programatically</source></property>
<property><name>io.compression.codecs</name><value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value><source>programatically</source></property>
<property><name>io.file.buffer.size</name><value>4096</value><source>programatically</source></property>
<property><name>mapred.map.tasks.speculative.execution</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.job.split.metainfo.maxsize</name><value>10000000</value><source>programatically</source></property>
<property><name>mapred.map.max.attempts</name><value>4</value><source>programatically</source></property>
<property><name>mapred.job.shuffle.merge.percent</name><value>0.66</value><source>programatically</source></property>
<property><name>fs.har.impl</name><value>org.apache.hadoop.fs.HarFileSystem</value><source>programatically</source></property>
<property><name>hadoop.security.authentication</name><value>simple</value><source>programatically</source></property>
<property><name>fs.s3.buffer.dir</name><value>${hadoop.tmp.dir}/s3</value><source>programatically</source></property>
<property><name>mapred.skip.reduce.auto.incr.proc.count</name><value>true</value><source>programatically</source></property>
<property><name>dfs.http.address</name><value>0.0.0.0:50070</value><source>programatically</source></property>
<property><name>eclipse.plug-in.socks.proxy.enable</name><value>no</value><source>programatically</source></property>
<property><name>mapred.job.tracker.jobhistory.lru.cache.size</name><value>5</value><source>programatically</source></property>
<property><name>rpc.metrics.quantile.enable</name><value>false</value><source>programatically</source></property>
<property><name>eclipse.plug-in.namenode.host</name><value>hadmad01.homedepot.com</value><source>programatically</source></property>
<property><name>dfs.replication.considerLoad</name><value>true</value><source>programatically</source></property>
<property><name>mapred.jobtracker.blacklist.fault-bucket-width</name><value>15</value><source>programatically</source></property>
<property><name>tfile.fs.input.buffer.size</name><value>262144</value><source>programatically</source></property>
<property><name>dfs.block.access.token.enable</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.job.acl-view-job</name><value> </value><source>programatically</source></property>
<property><name>mapred.job.queue.name</name><value>default</value><source>programatically</source></property>
<property><name>ftp.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>ha.failover-controller.cli-check.rpc-timeout.ms</name><value>20000</value><source>programatically</source></property>
<property><name>dfs.permissions</name><value>true</value><source>programatically</source></property>
<property><name>mapred.job.tracker.persist.jobstatus.hours</name><value>0</value><source>programatically</source></property>
<property><name>fs.file.impl</name><value>org.apache.hadoop.fs.LocalFileSystem</value><source>programatically</source></property>
<property><name>dfs.block.size</name><value>134217728</value><source>programatically</source></property>
<property><name>hadoop.socks.server</name><value>host:1080</value><source>programatically</source></property>
<property><name>dfs.https.address</name><value>0.0.0.0:50470</value><source>programatically</source></property>
<property><name>ipc.client.kill.max</name><value>10</value><source>programatically</source></property>
<property><name>mapred.healthChecker.script.timeout</name><value>600000</value><source>programatically</source></property>
<property><name>mapred.tasktracker.map.tasks.maximum</name><value>2</value><source>programatically</source></property>
<property><name>mapred.job.tracker.persist.jobstatus.dir</name><value>/jobtracker/jobsInfo</value><source>programatically</source></property>
<property><name>dfs.default.chunk.view.size</name><value>32768</value><source>programatically</source></property>
<property><name>mapred.reduce.slowstart.completed.maps</name><value>0.05</value><source>programatically</source></property>
<property><name>hadoop.http.filter.initializers</name><value>org.apache.hadoop.http.lib.StaticUserWebFilter</value><source>programatically</source></property>
<property><name>eclipse.plug-in.socks.proxy.port</name><value>1080</value><source>programatically</source></property>
<property><name>io.sort.mb</name><value>100</value><source>programatically</source></property>
<property><name>dfs.datanode.failed.volumes.tolerated</name><value>0</value><source>programatically</source></property>
<property><name>dfs.https.need.client.auth</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.type</name><value>simple</value><source>programatically</source></property>
<property><name>dfs.datanode.data.dir.perm</name><value>755</value><source>programatically</source></property>
<property><name>ipc.server.listen.queue.size</name><value>128</value><source>programatically</source></property>
<property><name>file.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>io.mapfile.bloom.size</name><value>1048576</value><source>programatically</source></property>
<property><name>fs.hsftp.impl</name><value>org.apache.hadoop.hdfs.HsftpFileSystem</value><source>programatically</source></property>
<property><name>fs.swift.impl</name><value>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</value><source>programatically</source></property>
<property><name>mapred.combine.recordsBeforeProgress</name><value>10000</value><source>programatically</source></property>
<property><name>ftp.replication</name><value>3</value><source>programatically</source></property>
<property><name>dfs.datanode.dns.nameserver</name><value>default</value><source>programatically</source></property>
<property><name>mapred.child.java.opts</name><value>-Xmx200m</value><source>programatically</source></property>
<property><name>dfs.replication.max</name><value>512</value><source>programatically</source></property>
<property><name>mapred.queue.default.state</name><value>RUNNING</value><source>programatically</source></property>
<property><name>map.sort.class</name><value>org.apache.hadoop.util.QuickSort</value><source>programatically</source></property>
<property><name>hadoop.util.hash.type</name><value>murmur</value><source>programatically</source></property>
<property><name>topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value><source>programatically</source></property>
<property><name>dfs.block.access.key.update.interval</name><value>600</value><source>programatically</source></property>
<property><name>dfs.datanode.dns.interface</name><value>default</value><source>programatically</source></property>
<property><name>dfs.datanode.use.datanode.hostname</name><value>false</value><source>programatically</source></property>
<property><name>mapred.output.compression.type</name><value>RECORD</value><source>programatically</source></property>
<property><name>hadoop.security.use-weak-http-crypto</name><value>false</value><source>programatically</source></property>
<property><name>mapred.skip.attempts.to.start.skipping</name><value>2</value><source>programatically</source></property>
<property><name>ha.zookeeper.acl</name><value>world:anyone:rwcda</value><source>programatically</source></property>
<property><name>io.map.index.skip</name><value>0</value><source>programatically</source></property>
<property><name>dfs.namenode.check.stale.datanode</name><value>false</value><source>programatically</source></property>
<property><name>net.topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value><source>programatically</source></property>
<property><name>mapred.cluster.max.map.memory.mb</name><value>-1</value><source>programatically</source></property>
<property><name>fs.s3.maxRetries</name><value>4</value><source>programatically</source></property>
<property><name>dfs.namenode.logging.level</name><value>info</value><source>programatically</source></property>
<property><name>ha.failover-controller.new-active.rpc-timeout.ms</name><value>60000</value><source>programatically</source></property>
<property><name>s3native.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>mapred.task.tracker.task-controller</name><value>org.apache.hadoop.mapred.DefaultTaskController</value><source>programatically</source></property>
<property><name>mapred.userlog.limit.kb</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.http.staticuser.user</name><value>dr.who</value><source>programatically</source></property>
<property><name>mapreduce.ifile.readahead.bytes</name><value>4194304</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.simple.anonymous.allowed</name><value>true</value><source>programatically</source></property>
<property><name>hadoop.rpc.socket.factory.class.default</name><value>org.apache.hadoop.net.StandardSocketFactory</value><source>programatically</source></property>
<property><name>fs.hftp.impl</name><value>org.apache.hadoop.hdfs.HftpFileSystem</value><source>programatically</source></property>
<property><name>dfs.namenode.handler.count</name><value>10</value><source>programatically</source></property>
<property><name>fs.automatic.close</name><value>true</value><source>programatically</source></property>
<property><name>fs.kfs.impl</name><value>org.apache.hadoop.fs.kfs.KosmosFileSystem</value><source>programatically</source></property>
<property><name>mapred.map.tasks</name><value>2</value><source>programatically</source></property>
<property><name>mapred.local.dir.minspacekill</name><value>0</value><source>programatically</source></property>
<property><name>fs.hdfs.impl</name><value>org.apache.hadoop.hdfs.DistributedFileSystem</value><source>programatically</source></property>
<property><name>mapred.job.map.memory.mb</name><value>-1</value><source>programatically</source></property>
<property><name>mapred.jobtracker.completeuserjobs.maximum</name><value>100</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.directory.search.timeout</name><value>10000</value><source>programatically</source></property>
<property><name>ftp.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>ha.health-monitor.rpc-timeout.ms</name><value>45000</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.attr.member</name><value>member</value><source>programatically</source></property>
<property><name>dfs.blockreport.initialDelay</name><value>0</value><source>programatically</source></property>
<property><name>mapred.min.split.size</name><value>0</value><source>programatically</source></property>
<property><name>io.compression.codec.bzip2.library</name><value>system-native</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.token.validity</name><value>36000</value><source>programatically</source></property>
<property><name>dfs.namenode.delegation.token.max-lifetime</name><value>604800000</value><source>programatically</source></property>
<property><name>fs.ftp.impl</name><value>org.apache.hadoop.fs.ftp.FTPFileSystem</value><source>programatically</source></property>
<property><name>dfs.secondary.http.address</name><value>0.0.0.0:50090</value><source>programatically</source></property>
<property><name>mapred.output.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value><source>programatically</source></property>
<property><name>mapred.cluster.max.reduce.memory.mb</name><value>-1</value><source>programatically</source></property>
<property><name>mapred.cluster.reduce.memory.mb</name><value>-1</value><source>programatically</source></property>
<property><name>s3native.replication</name><value>3</value><source>programatically</source></property>
<property><name>dfs.web.ugi</name><value>webuser,webgroup</value><source>programatically</source></property>
<property><name>mapred.task.profile</name><value>false</value><source>programatically</source></property>
<property><name>mapred.reduce.parallel.copies</name><value>5</value><source>programatically</source></property>
<property><name>dfs.heartbeat.interval</name><value>3</value><source>programatically</source></property>
<property><name>dfs.ha.fencing.ssh.connect-timeout</name><value>30000</value><source>programatically</source></property>
<property><name>net.topology.impl</name><value>org.apache.hadoop.net.NetworkTopology</value><source>programatically</source></property>
<property><name>local.cache.size</name><value>10737418240</value><source>programatically</source></property>
<property><name>io.sort.factor</name><value>10</value><source>programatically</source></property>
<property><name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name><value>5000</value><source>programatically</source></property>
<property><name>eclipse.plug-in.jobtracker.host</name><value>hadmad02.homedepot.com</value><source>programatically</source></property>
<property><name>mapreduce.job.counters.groups.max</name><value>50</value><source>programatically</source></property>
<property><name>mapred.task.timeout</name><value>600000</value><source>programatically</source></property>
<property><name>eclipse.plug-in.namenode.port</name><value>8020</value><source>programatically</source></property>
<property><name>dfs.safemode.extension</name><value>30000</value><source>programatically</source></property>
<property><name>ipc.client.idlethreshold</name><value>4000</value><source>programatically</source></property>
<property><name>ipc.server.tcpnodelay</name><value>false</value><source>programatically</source></property>
<property><name>ftp.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>hadoop.logfile.count</name><value>10</value><source>programatically</source></property>
<property><name>dfs.namenode.stale.datanode.interval</name><value>30000</value><source>programatically</source></property>
<property><name>s3.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>mapred.heartbeats.in.second</name><value>100</value><source>programatically</source></property>
<property><name>fs.s3.block.size</name><value>67108864</value><source>programatically</source></property>
<property><name>mapred.map.output.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value><source>programatically</source></property>
<property><name>hadoop.rpc.protection</name><value>authentication</value><source>programatically</source></property>
<property><name>mapred.task.cache.levels</name><value>2</value><source>programatically</source></property>
<property><name>mapred.tasktracker.dns.interface</name><value>default</value><source>programatically</source></property>
<property><name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name><value>${dfs.web.authentication.kerberos.principal}</value><source>programatically</source></property>
<property><name>ftp.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>fs.defaultFS</name><value>hdfs://hadmad01.homedepot.com:8020/</value><source>because fs.default.name is deprecated</source></property>
<property><name>file.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>mapred.job.reduce.memory.mb</name><value>-1</value><source>programatically</source></property>
<property><name>mapred.max.tracker.failures</name><value>4</value><source>programatically</source></property>
<property><name>fs.trash.checkpoint.interval</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.signature.secret.file</name><value>${user.home}/hadoop-http-auth-signature-secret</value><source>programatically</source></property>
<property><name>s3native.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>dfs.df.interval</name><value>60000</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.read.timeout</name><value>180000</value><source>programatically</source></property>
<property><name>mapred.tasktracker.tasks.sleeptime-before-sigkill</name><value>5000</value><source>programatically</source></property>
<property><name>mapred.max.tracker.blacklists</name><value>4</value><source>programatically</source></property>
<property><name>fs.permissions.umask-mode</name><value>022</value><source>programatically</source></property>
<property><name>hadoop.common.configuration.version</name><value>0.23.0</value><source>programatically</source></property>
<property><name>jobclient.output.filter</name><value>FAILED</value><source>programatically</source></property>
<property><name>mapreduce.ifile.readahead</name><value>true</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.ssl</name><value>false</value><source>programatically</source></property>
<property><name>io.serializations</name><value>org.apache.hadoop.io.serializer.WritableSerialization</value><source>programatically</source></property>
<property><name>fs.df.interval</name><value>60000</value><source>programatically</source></property>
<property><name>job.end.retry.attempts</name><value>0</value><source>programatically</source></property>
<property><name>mapred.jobtracker.taskScheduler</name><value>org.apache.hadoop.mapred.JobQueueTaskScheduler</value><source>programatically</source></property>
<property><name>io.seqfile.compress.blocksize</name><value>1000000</value><source>programatically</source></property>
<property><name>ipc.client.connect.max.retries</name><value>10</value><source>programatically</source></property>
<property><name>hadoop.security.groups.cache.secs</name><value>300</value><source>programatically</source></property>
<property><name>dfs.namenode.delegation.key.update-interval</name><value>86400000</value><source>programatically</source></property>
<property><name>webinterface.private.actions</name><value>false</value><source>programatically</source></property>
<property><name>mapred.tasktracker.indexcache.mb</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.reduce.input.limit</name><value>-1</value><source>programatically</source></property>
<property><name>fs.checkpoint.edits.dir</name><value>${fs.checkpoint.dir}</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.filter.user</name><value>(&amp;(objectClass=user)(sAMAccountName={0}))</value><source>programatically</source></property>
<property><name>tasktracker.http.threads</name><value>40</value><source>programatically</source></property>
<property><name>dfs.namenode.kerberos.internal.spnego.principal</name><value>${dfs.web.authentication.kerberos.principal}</value><source>programatically</source></property>
<property><name>fs.s3n.multipart.copy.block.size</name><value>5368709120</value><source>programatically</source></property>
<property><name>fs.s3n.block.size</name><value>67108864</value><source>programatically</source></property>
<property><name>mapred.job.tracker.handler.count</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.job.counters.group.name.max</name><value>128</value><source>programatically</source></property>
<property><name>mapred.output.compress</name><value>false</value><source>programatically</source></property>
<property><name>keep.failed.task.files</name><value>false</value><source>programatically</source></property>
<property><name>fs.ftp.host</name><value>0.0.0.0</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping</name><value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value><source>programatically</source></property>
<property><name>dfs.https.client.keystore.resource</name><value>ssl-client.xml</value><source>programatically</source></property>
<property><name>mapred.jobtracker.job.history.block.size</name><value>3145728</value><source>programatically</source></property>
<property><name>mapred.skip.reduce.max.skip.groups</name><value>0</value><source>programatically</source></property>
<property><name>dfs.datanode.address</name><value>0.0.0.0:50010</value><source>programatically</source></property>
<property><name>dfs.datanode.max.xcievers</name><value>4096</value><source>programatically</source></property>
<property><name>dfs.datanode.https.address</name><value>0.0.0.0:50475</value><source>programatically</source></property>
<property><name>fs.s3.impl</name><value>org.apache.hadoop.fs.s3.S3FileSystem</value><source>programatically</source></property>
<property><name>file.replication</name><value>1</value><source>programatically</source></property>
<property><name>hadoop.work.around.non.threadsafe.getpwuid</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.tmp.dir</name><value>/tmp/hadoop-${user.name}</value><source>programatically</source></property>
<property><name>mapred.line.input.format.linespermap</name><value>1</value><source>programatically</source></property>
<property><name>hadoop.kerberos.kinit.command</name><value>kinit</value><source>programatically</source></property>
<property><name>topology.script.number.args</name><value>100</value><source>programatically</source></property>
<property><name>dfs.datanode.du.reserved</name><value>0</value><source>programatically</source></property>
<property><name>fs.default.name</name><value>hdfs://hadmad01.homedepot.com:8020/</value></property>
<property><name>dfs.balance.bandwidthPerSec</name><value>1048576</value><source>programatically</source></property>
<property><name>file.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>mapred.local.dir.minspacestart</name><value>0</value><source>programatically</source></property>
<property><name>mapred.jobtracker.maxtasks.per.job</name><value>-1</value><source>programatically</source></property>
<property><name>mapred.reduce.max.attempts</name><value>4</value><source>programatically</source></property>
<property><name>mapred.user.jobconf.limit</name><value>5242880</value><source>programatically</source></property>
<property><name>net.topology.script.number.args</name><value>100</value><source>programatically</source></property>
<property><name>dfs.namenode.decommission.interval</name><value>30</value><source>programatically</source></property>
<property><name>mapred.job.tracker</name><value>hadmad02.homedepot.com:54311</value><source>programatically</source></property>
<property><name>dfs.name.edits.dir</name><value>${dfs.name.dir}</value><source>programatically</source></property>
<property><name>hadoop.ssl.hostname.verifier</name><value>DEFAULT</value><source>programatically</source></property>
<property><name>ipc.client.connect.timeout</name><value>20000</value><source>programatically</source></property>
<property><name>io.mapfile.bloom.error.rate</name><value>0.005</value><source>programatically</source></property>
<property><name>mapred.tasktracker.expiry.interval</name><value>600000</value><source>programatically</source></property>
<property><name>dfs.safemode.threshold.pct</name><value>0.999f</value><source>programatically</source></property>
<property><name>io.sort.record.percent</name><value>0.05</value><source>programatically</source></property>
<property><name>mapred.job.tracker.persist.jobstatus.active</name><value>false</value><source>programatically</source></property>
<property><name>ha.failover-controller.graceful-fence.connection.retries</name><value>1</value><source>programatically</source></property>
<property><name>ha.health-monitor.connect-retry-interval.ms</name><value>1000</value><source>programatically</source></property>
<property><name>io.seqfile.local.dir</name><value>${hadoop.tmp.dir}/io/local</value><source>programatically</source></property>
<property><name>tfile.io.chunk.size</name><value>1048576</value><source>programatically</source></property>
<property><name>file.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>dfs.name.dir</name><value>${hadoop.tmp.dir}/dfs/name</value><source>programatically</source></property>
<property><name>fs.checkpoint.period</name><value>3600</value><source>programatically</source></property>
<property><name>mapreduce.job.acl-modify-job</name><value> </value><source>programatically</source></property>
<property><name>io.skip.checksum.errors</name><value>false</value><source>programatically</source></property>
<property><name>dfs.datanode.handler.count</name><value>3</value><source>programatically</source></property>
<property><name>dfs.namenode.decommission.nodes.per.interval</name><value>5</value><source>programatically</source></property>
<property><name>fs.ftp.host.port</name><value>21</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.viewfs.impl</name><value>org.apache.hadoop.fs.viewfs.ViewFs</value><source>programatically</source></property>
<property><name>mapred.temp.dir</name><value>${hadoop.tmp.dir}/mapred/temp</value><source>programatically</source></property>
<property><name>hadoop.native.lib</name><value>true</value></property>
<property><name>ipc.client.fallback-to-simple-auth-allowed</name><value>false</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.wasb.impl</name><value>org.apache.hadoop.fs.azurenative.Wasb</value><source>programatically</source></property>
<property><name>dfs.datanode.ipc.address</name><value>0.0.0.0:50020</value><source>programatically</source></property>
<property><name>fs.webhdfs.impl</name><value>org.apache.hadoop.hdfs.web.WebHdfsFileSystem</value><source>programatically</source></property>
<property><name>hadoop.ssl.require.client.cert</name><value>false</value><source>programatically</source></property>
</configuration>